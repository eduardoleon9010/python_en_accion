{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Asignaci贸n: Modelo de Regresi贸n con Keras**\n",
        "\n",
        "## **1. Tema de la asignaci贸n**\n",
        "\n",
        "En este proyecto, construiremos un modelo de **regresi贸n** utilizando la biblioteca **Keras** para modelar los datos sobre la **resistencia a la compresi贸n del hormig贸n**. Este ejercicio es una continuaci贸n del an谩lisis realizado en el **Laboratorio 3**, aplicando redes neuronales artificiales para predecir la resistencia del material en funci贸n de sus componentes.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Datos del hormig贸n**\n",
        "\n",
        "Para su comodidad, los datos se pueden encontrar en el siguiente enlace:\n",
        "\n",
        " [Descargar datos](https://cocl.us/concrete_data)\n",
        "\n",
        "Los datos contienen informaci贸n sobre la resistencia del hormig贸n en funci贸n de los siguientes predictores:\n",
        "\n",
        "- **Cemento**\n",
        "- **Escoria de alto horno**\n",
        "- **Cenizas volantes**\n",
        "- **Agua**\n",
        "- **Superplastificante**\n",
        "- **rido grueso**\n",
        "- **rido fino**\n",
        "\n",
        "El objetivo es construir un modelo que prediga la **resistencia a la compresi贸n** del hormig贸n bas谩ndose en estos factores.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Instrucciones para la tarea**\n",
        "\n",
        "1. **Carga de datos**: Importar y visualizar el conjunto de datos para entender su estructura.\n",
        "2. **Preprocesamiento**: Normalizaci贸n y divisi贸n en conjuntos de entrenamiento y prueba.\n",
        "3. **Construcci贸n del modelo**: Implementaci贸n de una red neuronal con Keras para regresi贸n.\n",
        "4. **Entrenamiento y evaluaci贸n**: Comparar el rendimiento del modelo a trav茅s de m茅tricas adecuadas, como el error medio cuadr谩tico (*MSE*).\n",
        "5. **Discusi贸n de resultados**: Comparar la diferencia en la media de los errores obtenidos entre distintas configuraciones del modelo.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "udl4U84cTeY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. C贸digo de implementaci贸n**"
      ],
      "metadata": {
        "id": "FerRr5eaT0pI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necesarias\n",
        "\n",
        "# pandas: Librer铆a para manipulaci贸n y an谩lisis de datos estructurados (DataFrames y Series)\n",
        "import pandas as pd\n",
        "\n",
        "# numpy: Librer铆a para operaciones num茅ricas y manejo de matrices/arreglos\n",
        "import numpy as np\n",
        "\n",
        "# tensorflow: Marco de trabajo para el desarrollo de modelos de aprendizaje profundo\n",
        "import tensorflow as tf\n",
        "\n",
        "# keras: API de alto nivel dentro de TensorFlow para construir y entrenar redes neuronales\n",
        "from tensorflow import keras\n",
        "\n",
        "# Sequential: Clase de Keras que permite construir modelos de redes neuronales capa por capa\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Dense: Capa de neuronas totalmente conectada utilizada en redes neuronales artificiales\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# train_test_split: Funci贸n de Scikit-learn para dividir los datos en conjuntos de entrenamiento y prueba\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# StandardScaler: M茅todo para normalizar los datos asegurando que tengan media 0 y desviaci贸n est谩ndar 1\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# mean_squared_error: M茅trica para evaluar la precisi贸n del modelo comparando valores predichos con los reales\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "vraDnoRKT505"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la funci贸n Input de Keras para definir expl铆citamente la capa de entrada\n",
        "from tensorflow.keras import Input"
      ],
      "metadata": {
        "id": "hRkprRIrU_BR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos\n",
        "data_url = \"https://cocl.us/concrete_data\"\n",
        "df = pd.read_csv(data_url)"
      ],
      "metadata": {
        "id": "Fr25jYRFUPsZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar variables predictoras y variable objetivo\n",
        "X = df.drop(columns=['Strength'])  # Variables de entrada\n",
        "y = df['Strength']  # Variable objetivo"
      ],
      "metadata": {
        "id": "pQfcFFsPURjt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar lista para almacenar los errores\n",
        "mse_list = []"
      ],
      "metadata": {
        "id": "fI2KGVtuUUrP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repetir el proceso 50 veces\n",
        "for _ in range(50):\n",
        "    # Dividir los datos en conjunto de entrenamiento (70%) y prueba (30%)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
        "\n",
        "    # Normalizar los datos\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Construcci贸n del modelo con la recomendaci贸n de Keras\n",
        "    model = Sequential([\n",
        "        Input(shape=(X_train.shape[1],)),  # Definir la capa de entrada expl铆citamente\n",
        "        Dense(64, activation='relu'),  # Primera capa oculta con 64 neuronas y ReLU\n",
        "        Dense(64, activation='relu'),  # Segunda capa oculta con 64 neuronas y ReLU\n",
        "        Dense(1)  # Capa de salida con 1 neurona para regresi贸n\n",
        "    ])\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Entrenar el modelo con 50 茅pocas\n",
        "    model.fit(X_train, y_train, epochs=50, verbose=0, batch_size=10)\n",
        "\n",
        "    # Evaluaci贸n del modelo\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_list.append(mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqnC3QHvUVhP",
        "outputId": "455933ab-e227-4664-fe25-ffd2fe3e4d13"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la media y desviaci贸n est谩ndar de los errores\n",
        "mse_mean = np.mean(mse_list)\n",
        "mse_std = np.std(mse_list)"
      ],
      "metadata": {
        "id": "aLZjs3uFUZIF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Error Medio Cuadr谩tico promedio: {mse_mean:.4f}')\n",
        "print(f'Desviaci贸n est谩ndar del MSE: {mse_std:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjC4xboVUN3t",
        "outputId": "055cbec4-ec78-4ae4-8917-96301526330e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Medio Cuadr谩tico promedio: 49.6710\n",
            "Desviaci贸n est谩ndar del MSE: 25.4673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusi贸n\n",
        "\n",
        "En este experimento, se construy贸 un modelo de red neuronal para predecir la resistencia a la compresi贸n del hormig贸n a partir de sus componentes. Tras realizar 50 repeticiones del proceso de entrenamiento y evaluaci贸n, se obtuvo un error medio cuadr谩tico (MSE) promedio de **49.6710** con una desviaci贸n est谩ndar de **25.4673**.\n",
        "\n",
        "Estos resultados indican que el modelo tiene un desempe帽o estable; sin embargo, la alta desviaci贸n est谩ndar sugiere que la precisi贸n de las predicciones puede verse afectada por la variabilidad en la selecci贸n del conjunto de entrenamiento. Para mejorar su rendimiento, podr铆an implementarse estrategias como el ajuste de hiperpar谩metros, el uso de arquitecturas m谩s complejas o el aumento del tama帽o del conjunto de datos.\n",
        "\n",
        "La red neuronal construida logra modelar la relaci贸n entre los componentes del hormig贸n y su resistencia, pero a煤n presenta margen de mejora para optimizar su precisi贸n y reducir la variabilidad en sus predicciones.\n"
      ],
      "metadata": {
        "id": "72Gqqa_0Zz_p"
      }
    }
  ]
}