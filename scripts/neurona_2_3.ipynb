{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **A. Modelo de Regresi贸n con Keras**\n",
        "\n",
        "### **1. Tema de la asignaci贸n**\n",
        "\n",
        "En este proyecto, construiremos un modelo de **regresi贸n** utilizando la biblioteca **Keras** para modelar los datos sobre la **resistencia a la compresi贸n del hormig贸n**. Este ejercicio es una continuaci贸n del an谩lisis realizado en el **Laboratorio 3**, aplicando redes neuronales artificiales para predecir la resistencia del material en funci贸n de sus componentes.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Datos del hormig贸n**\n",
        "\n",
        "Para su comodidad, los datos se pueden encontrar en el siguiente enlace:\n",
        "\n",
        " [Descargar datos](https://cocl.us/concrete_data)\n",
        "\n",
        "Los datos contienen informaci贸n sobre la resistencia del hormig贸n en funci贸n de los siguientes predictores:\n",
        "\n",
        "- **Cemento**\n",
        "- **Escoria de alto horno**\n",
        "- **Cenizas volantes**\n",
        "- **Agua**\n",
        "- **Superplastificante**\n",
        "- **rido grueso**\n",
        "- **rido fino**\n",
        "\n",
        "El objetivo es construir un modelo que prediga la **resistencia a la compresi贸n** del hormig贸n bas谩ndose en estos factores.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Instrucciones para la tarea**\n",
        "\n",
        "1. **Carga de datos**: Importar y visualizar el conjunto de datos para entender su estructura.\n",
        "2. **Preprocesamiento**: Normalizaci贸n y divisi贸n en conjuntos de entrenamiento y prueba.\n",
        "3. **Construcci贸n del modelo**: Implementaci贸n de una red neuronal con Keras para regresi贸n.\n",
        "4. **Entrenamiento y evaluaci贸n**: Comparar el rendimiento del modelo a trav茅s de m茅tricas adecuadas, como el error medio cuadr谩tico (*MSE*).\n",
        "5. **Discusi贸n de resultados**: Comparar la diferencia en la media de los errores obtenidos entre distintas configuraciones del modelo.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "udl4U84cTeY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. C贸digo de implementaci贸n**"
      ],
      "metadata": {
        "id": "FerRr5eaT0pI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necesarias\n",
        "\n",
        "# pandas: Librer铆a para manipulaci贸n y an谩lisis de datos estructurados (DataFrames y Series)\n",
        "import pandas as pd\n",
        "\n",
        "# numpy: Librer铆a para operaciones num茅ricas y manejo de matrices/arreglos\n",
        "import numpy as np\n",
        "\n",
        "# tensorflow: Marco de trabajo para el desarrollo de modelos de aprendizaje profundo\n",
        "import tensorflow as tf\n",
        "\n",
        "# keras: API de alto nivel dentro de TensorFlow para construir y entrenar redes neuronales\n",
        "from tensorflow import keras\n",
        "\n",
        "# Sequential: Clase de Keras que permite construir modelos de redes neuronales capa por capa\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Dense: Capa de neuronas totalmente conectada utilizada en redes neuronales artificiales\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# train_test_split: Funci贸n de Scikit-learn para dividir los datos en conjuntos de entrenamiento y prueba\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# StandardScaler: M茅todo para normalizar los datos asegurando que tengan media 0 y desviaci贸n est谩ndar 1\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# mean_squared_error: M茅trica para evaluar la precisi贸n del modelo comparando valores predichos con los reales\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "vraDnoRKT505"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la funci贸n Input de Keras para definir expl铆citamente la capa de entrada\n",
        "from tensorflow.keras import Input"
      ],
      "metadata": {
        "id": "hRkprRIrU_BR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos\n",
        "data_url = \"https://cocl.us/concrete_data\"\n",
        "df = pd.read_csv(data_url)"
      ],
      "metadata": {
        "id": "Fr25jYRFUPsZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar variables predictoras y variable objetivo\n",
        "X = df.drop(columns=['Strength'])  # Variables de entrada\n",
        "y = df['Strength']  # Variable objetivo"
      ],
      "metadata": {
        "id": "pQfcFFsPURjt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar lista para almacenar los errores\n",
        "mse_list = []"
      ],
      "metadata": {
        "id": "fI2KGVtuUUrP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repetir el proceso 50 veces\n",
        "for _ in range(50):\n",
        "    # Dividir los datos en conjunto de entrenamiento (70%) y prueba (30%)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
        "\n",
        "    # Normalizar los datos\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Construcci贸n del modelo con la recomendaci贸n de Keras\n",
        "    model = Sequential([\n",
        "        Input(shape=(X_train.shape[1],)),  # Definir la capa de entrada expl铆citamente\n",
        "        Dense(64, activation='relu'),  # Primera capa oculta con 64 neuronas y ReLU\n",
        "        Dense(64, activation='relu'),  # Segunda capa oculta con 64 neuronas y ReLU\n",
        "        Dense(1)  # Capa de salida con 1 neurona para regresi贸n\n",
        "    ])\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Entrenar el modelo con 50 茅pocas\n",
        "    model.fit(X_train, y_train, epochs=50, verbose=0, batch_size=10)\n",
        "\n",
        "    # Evaluaci贸n del modelo\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_list.append(mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqnC3QHvUVhP",
        "outputId": "455933ab-e227-4664-fe25-ffd2fe3e4d13"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la media y desviaci贸n est谩ndar de los errores\n",
        "mse_mean = np.mean(mse_list)\n",
        "mse_std = np.std(mse_list)"
      ],
      "metadata": {
        "id": "aLZjs3uFUZIF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Error Medio Cuadr谩tico promedio: {mse_mean:.4f}')\n",
        "print(f'Desviaci贸n est谩ndar del MSE: {mse_std:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjC4xboVUN3t",
        "outputId": "055cbec4-ec78-4ae4-8917-96301526330e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Medio Cuadr谩tico promedio: 49.6710\n",
            "Desviaci贸n est谩ndar del MSE: 25.4673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusi贸n\n",
        "\n",
        "En este experimento, se construy贸 un modelo de red neuronal para predecir la resistencia a la compresi贸n del hormig贸n a partir de sus componentes. Tras realizar 50 repeticiones del proceso de entrenamiento y evaluaci贸n, se obtuvo un error medio cuadr谩tico (MSE) promedio de **49.6710** con una desviaci贸n est谩ndar de **25.4673**.\n",
        "\n",
        "Estos resultados indican que el modelo tiene un desempe帽o estable; sin embargo, la alta desviaci贸n est谩ndar sugiere que la precisi贸n de las predicciones puede verse afectada por la variabilidad en la selecci贸n del conjunto de entrenamiento. Para mejorar su rendimiento, podr铆an implementarse estrategias como el ajuste de hiperpar谩metros, el uso de arquitecturas m谩s complejas o el aumento del tama帽o del conjunto de datos.\n",
        "\n",
        "La red neuronal construida logra modelar la relaci贸n entre los componentes del hormig贸n y su resistencia, pero a煤n presenta margen de mejora para optimizar su precisi贸n y reducir la variabilidad en sus predicciones.\n"
      ],
      "metadata": {
        "id": "72Gqqa_0Zz_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**B. Implementaci贸n con Datos Normalizados**\n",
        "\n"
      ],
      "metadata": {
        "id": "n16sbgWsac1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar lista para almacenar los errores con datos normalizados\n",
        "mse_list_normalized = []"
      ],
      "metadata": {
        "id": "sfaO865UahXB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repetir el proceso 50 veces con datos normalizados\n",
        "for _ in range(50):\n",
        "    # Dividir los datos en conjunto de entrenamiento (70%) y prueba (30%)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
        "\n",
        "    # Normalizar los datos (restar la media y dividir por la desviaci贸n est谩ndar)\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Construcci贸n del modelo corregido\n",
        "    model = Sequential([\n",
        "        Input(shape=(X_train.shape[1],)),  # Definir la entrada expl铆citamente\n",
        "        Dense(10, activation='relu'),  # Primera capa oculta\n",
        "        Dense(10, activation='relu'),  # Segunda capa oculta\n",
        "        Dense(10, activation='relu'),  # Tercera capa oculta\n",
        "        Dense(1)  # Capa de salida para regresi贸n\n",
        "    ])\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Entrenar el modelo con 50 茅pocas\n",
        "    model.fit(X_train, y_train, epochs=50, verbose=0, batch_size=10)\n",
        "\n",
        "    # Evaluaci贸n del modelo\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_list_normalized.append(mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBRvtimsa-zZ",
        "outputId": "47f05f37-3a8d-4162-d261-f4be77d801f8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la media y desviaci贸n est谩ndar de los errores con datos normalizados\n",
        "mse_mean_normalized = np.mean(mse_list_normalized)\n",
        "mse_std_normalized = np.std(mse_list_normalized)\n",
        "\n",
        "print(f'Error Medio Cuadr谩tico promedio con normalizaci贸n: {mse_mean_normalized:.4f}')\n",
        "print(f'Desviaci贸n est谩ndar del MSE con normalizaci贸n: {mse_std_normalized:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDGF7faQbEHw",
        "outputId": "c7b61bc5-d7b9-439d-f786-a495f7a2c277"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Medio Cuadr谩tico promedio con normalizaci贸n: 89.6288\n",
            "Desviaci贸n est谩ndar del MSE con normalizaci贸n: 22.6291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparaci贸n de los resultados\n",
        "\n",
        "Comparando los errores medios al cuadrado de la primera implementaci贸n y de la versi贸n con datos normalizados:\n",
        "\n",
        "### Modelo sin normalizaci贸n:\n",
        "- **MSE promedio:** 49.6710  \n",
        "- **Desviaci贸n est谩ndar:** 25.4673  \n",
        "\n",
        "### Modelo con normalizaci贸n:\n",
        "- **MSE promedio:** 89.6288  \n",
        "- **Desviaci贸n est谩ndar:** 22.6291  \n",
        "\n",
        "## **Conclusi贸n**\n",
        "En este caso, la normalizaci贸n de los datos no mejor贸 el desempe帽o del modelo, ya que el MSE promedio aument贸 de **49.6710** a **89.6288**. Aunque la desviaci贸n est谩ndar disminuy贸 ligeramente, lo que indica una menor variabilidad en los errores, el incremento del MSE sugiere que la normalizaci贸n no favoreci贸 la capacidad predictiva de la red neuronal en esta implementaci贸n.  \n",
        "\n",
        "La normalizaci贸n suele mejorar la estabilidad y la convergencia del modelo en muchos escenarios, pero su efectividad depende del tipo de datos y del modelo utilizado. Para mejorar el rendimiento, podr铆a ser necesario ajustar otros hiperpar谩metros, explorar diferentes estrategias de normalizaci贸n o probar arquitecturas m谩s complejas.\n"
      ],
      "metadata": {
        "id": "pD9NRAt2eeri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **C. Implementaci贸n con 100 pocas**"
      ],
      "metadata": {
        "id": "jgIBDBcIfNvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar lista para almacenar los errores con 100 茅pocas\n",
        "mse_list_100_epochs = []\n",
        "\n",
        "# Repetir el proceso 50 veces con datos normalizados y 100 茅pocas\n",
        "for _ in range(50):\n",
        "    # Dividir los datos en conjunto de entrenamiento (70%) y prueba (30%)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
        "\n",
        "    # Normalizar los datos (restar la media y dividir por la desviaci贸n est谩ndar)\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Construcci贸n del modelo\n",
        "    model = Sequential([\n",
        "        Input(shape=(X_train.shape[1],)),  # Definir la entrada expl铆citamente\n",
        "        Dense(10, activation='relu'),  # Capa oculta con 10 nodos y ReLU\n",
        "        Dense(1)  # Capa de salida para regresi贸n\n",
        "    ])\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Entrenar el modelo con **100 茅pocas**\n",
        "    model.fit(X_train, y_train, epochs=100, verbose=0, batch_size=10)\n",
        "\n",
        "    # Evaluaci贸n del modelo\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_list_100_epochs.append(mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n8NSZ16fJvT",
        "outputId": "4305bda1-e933-472b-f248-34a96404d70c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la media y desviaci贸n est谩ndar de los errores con 100 茅pocas\n",
        "mse_mean_100_epochs = np.mean(mse_list_100_epochs)\n",
        "mse_std_100_epochs = np.std(mse_list_100_epochs)"
      ],
      "metadata": {
        "id": "mplFIMByghOJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Error Medio Cuadr谩tico promedio con 100 茅pocas: {mse_mean_100_epochs:.4f}')\n",
        "print(f'Desviaci贸n est谩ndar del MSE con 100 茅pocas: {mse_std_100_epochs:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mgyui34g1-5",
        "outputId": "3b07aacc-7aeb-4287-8f43-abc96f2dbef3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Medio Cuadr谩tico promedio con 100 茅pocas: 78.8172\n",
            "Desviaci贸n est谩ndar del MSE con 100 茅pocas: 16.1893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparaci贸n de los resultados\n",
        "\n",
        "Comparando los errores medios al cuadrado obtenidos en los pasos anteriores:\n",
        "\n",
        "#### Modelo con 50 茅pocas (Paso B - Normalizado):\n",
        "- **MSE promedio:** 89.6288  \n",
        "- **Desviaci贸n est谩ndar:** 22.6291  \n",
        "\n",
        "#### Modelo con 100 茅pocas (Paso C):\n",
        "- **MSE promedio:** 78.8172  \n",
        "- **Desviaci贸n est谩ndar:** 16.1893  \n",
        "\n",
        "## **Conclusi贸n**\n",
        "\n",
        "Aumentar el n煤mero de 茅pocas a **100** ha reducido el error medio cuadr谩tico de **89.6288** a **78.8172**, lo que indica que el modelo ha seguido aprendiendo y mejorando su desempe帽o con m谩s iteraciones. Adem谩s, la disminuci贸n en la desviaci贸n est谩ndar (**de 22.6291 a 16.1893**) sugiere que las predicciones son m谩s consistentes y menos sensibles a la selecci贸n del conjunto de entrenamiento.  \n",
        "\n",
        "Sin embargo, aunque el modelo ha mejorado con m谩s 茅pocas, es importante evaluar si existe riesgo de **sobreajuste**. Si el error en los datos de prueba deja de disminuir o comienza a aumentar en futuras iteraciones, podr铆a ser necesario aplicar estrategias como la **regularizaci贸n** o el **early stopping** para evitar que el modelo memorice los datos en lugar de generalizar correctamente.  \n",
        "\n",
        "En general, el aumento en las 茅pocas ha sido beneficioso en este caso, pero se recomienda seguir monitoreando el desempe帽o para determinar el punto 贸ptimo de entrenamiento.  "
      ],
      "metadata": {
        "id": "OLUcc39XmHeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **D. Implementaci贸n con tres capas ocultas**\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKWqOzJEmnAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar lista para almacenar los errores con tres capas ocultas\n",
        "mse_list_3_layers = []"
      ],
      "metadata": {
        "id": "jbKcIM5VmyvT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repetir el proceso 50 veces con datos normalizados y 3 capas ocultas\n",
        "for _ in range(50):\n",
        "    # Dividir los datos en conjunto de entrenamiento (70%) y prueba (30%)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
        "\n",
        "    # Normalizar los datos (restar la media y dividir por la desviaci贸n est谩ndar)\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Construcci贸n del modelo con m谩s capas ocultas\n",
        "    model = Sequential([\n",
        "        Input(shape=(X_train.shape[1],)),  # Definir la entrada expl铆citamente\n",
        "        Dense(10, activation='relu'),  # Primera capa oculta\n",
        "        Dense(10, activation='relu'),  # Segunda capa oculta\n",
        "        Dense(10, activation='relu'),  # Tercera capa oculta\n",
        "        Dense(1)  # Capa de salida para regresi贸n\n",
        "    ])\n",
        "\n",
        "    # Compilar el modelo\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Entrenar el modelo con **50 茅pocas**\n",
        "    model.fit(X_train, y_train, epochs=50, verbose=0, batch_size=10)\n",
        "\n",
        "    # Evaluaci贸n del modelo\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_list_3_layers.append(mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Foh4wExRm3Tw",
        "outputId": "fc54fd54-d253-4bf7-86df-c4726f651b94"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la media y desviaci贸n est谩ndar de los errores con tres capas ocultas\n",
        "mse_mean_3_layers = np.mean(mse_list_3_layers)\n",
        "mse_std_3_layers = np.std(mse_list_3_layers)"
      ],
      "metadata": {
        "id": "Hsep86TynnQw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Error Medio Cuadr谩tico promedio con 3 capas ocultas: {mse_mean_3_layers:.4f}')\n",
        "print(f'Desviaci贸n est谩ndar del MSE con 3 capas ocultas: {mse_std_3_layers:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uqDK6lgm4N5",
        "outputId": "71bbfd02-eb94-430e-f2dd-7a1888944584"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Medio Cuadr谩tico promedio con 3 capas ocultas: 83.1904\n",
            "Desviaci贸n est谩ndar del MSE con 3 capas ocultas: 25.1132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Comparaci贸n de los resultados**\n",
        "\n",
        "Comparando los errores medios al cuadrado obtenidos en los pasos anteriores:\n",
        "\n",
        "#### Modelo con 50 茅pocas (Paso B - Normalizado):\n",
        "- **MSE promedio:** 89.6288  \n",
        "- **Desviaci贸n est谩ndar:** 22.6291  \n",
        "\n",
        "#### Modelo con 100 茅pocas (Paso C):\n",
        "- **MSE promedio:** 78.8172  \n",
        "- **Desviaci贸n est谩ndar:** 16.1893  \n",
        "\n",
        "#### Modelo con 3 capas ocultas (Paso D):\n",
        "- **MSE promedio:** 83.1904  \n",
        "- **Desviaci贸n est谩ndar:** 25.1132  \n",
        "\n",
        "## **Conclusi贸n**\n",
        "\n",
        "Agregar **3 capas ocultas** ha resultado en un **MSE promedio de 83.1904**, lo que representa una leve mejora respecto al modelo con **50 茅pocas** (**89.6288**), pero un peor desempe帽o en comparaci贸n con el modelo entrenado por **100 茅pocas** (**78.8172**). Adem谩s, la **desviaci贸n est谩ndar aument贸** de **16.1893** a **25.1132**, lo que indica que las predicciones son menos estables.  \n",
        "\n",
        "Este resultado sugiere que, aunque una red m谩s profunda puede capturar patrones m谩s complejos, **no siempre garantiza una mejor generalizaci贸n**. La **mayor variabilidad en los errores** podr铆a ser indicativa de **sobreajuste**, donde el modelo se ajusta demasiado a los datos de entrenamiento sin mejorar significativamente su capacidad predictiva en datos nuevos.  \n",
        "\n",
        "Para mejorar el rendimiento, podr铆an explorarse estrategias como:  \n",
        "- **Ajuste de hiperpar谩metros**, optimizando el n煤mero de capas y neuronas.  \n",
        "- **Regularizaci贸n**, aplicando t茅cnicas como **Dropout** o penalizaci贸n **L2** para evitar sobreajuste.  \n",
        "- **Mayor cantidad de datos**, para mejorar la capacidad de generalizaci贸n del modelo.  \n",
        "\n",
        "Si bien agregar m谩s capas puede mejorar la capacidad de representaci贸n del modelo, **en este caso no ha superado el desempe帽o del modelo con 100 茅pocas**, por lo que se recomienda evaluar combinaciones 贸ptimas de arquitectura y estrategias de regularizaci贸n.  \n"
      ],
      "metadata": {
        "id": "GeRMeYcBqht2"
      }
    }
  ]
}